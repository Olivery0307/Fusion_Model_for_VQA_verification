THis is the repository for the Chung-Yeh Yang and Jin Kyu Cho's final project of Fall 2024 DS340, Fusion Model for VQA verification. Run the download_data.ipynb, 
preprocess_data, and main.ipynb in order to use the code and see the outcome. 

## Introduction
The problem we aimed to solve was determining whether a model could accurately classify a given answer to a question about an image as correct or incorrect. This challenge falls within the domain of Visual Question Answering (VQA), a task that combines computer vision and natural language processing.
The motivation for addressing this problem lies in the potential applications of VQA systems, such as assisting visually impaired individuals, improving automated customer service, and enhancing educational tools. However, accurately evaluating answers in a VQA context is complex, as it requires integrating visual and textual information to make nuanced decisions.
For this project, we developed the fusion model that evaluates the correctness of simple answers provided in response to questions about images. The dataset includes images paired with questions and answers, requiring the model to understand and interpret both modalities effectively. 

